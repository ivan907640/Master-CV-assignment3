[epoch  1, batch    50] loss: 2.305
[epoch  1, batch   100] loss: 2.302
[epoch  1, batch   150] loss: 2.303
[epoch  1, batch   200] loss: 2.301
[epoch  1, batch   250] loss: 2.299
[epoch  1, batch   300] loss: 2.298
[epoch  1, batch   350] loss: 2.297
[epoch  1, batch   400] loss: 2.295
[epoch  1, batch   450] loss: 2.292
[epoch  1, batch   500] loss: 2.290
[epoch  1, batch   550] loss: 2.287
[epoch  1, batch   600] loss: 2.283
[epoch  1, batch   650] loss: 2.278
[epoch  1, batch   700] loss: 2.273
[epoch  1, batch   750] loss: 2.263
[epoch  1, batch   800] loss: 2.243
[epoch  1, batch   850] loss: 2.218
[epoch  1, batch   900] loss: 2.165
[epoch  2, batch    50] loss: 1.860
[epoch  2, batch   100] loss: 1.351
[epoch  2, batch   150] loss: 0.888
[epoch  2, batch   200] loss: 0.692
[epoch  2, batch   250] loss: 0.597
[epoch  2, batch   300] loss: 0.556
[epoch  2, batch   350] loss: 0.525
[epoch  2, batch   400] loss: 0.497
[epoch  2, batch   450] loss: 0.469
[epoch  2, batch   500] loss: 0.467
[epoch  2, batch   550] loss: 0.451
[epoch  2, batch   600] loss: 0.430
[epoch  2, batch   650] loss: 0.436
[epoch  2, batch   700] loss: 0.428
[epoch  2, batch   750] loss: 0.401
[epoch  2, batch   800] loss: 0.410
[epoch  2, batch   850] loss: 0.362
[epoch  2, batch   900] loss: 0.391
[epoch  3, batch    50] loss: 0.404
[epoch  3, batch   100] loss: 0.373
[epoch  3, batch   150] loss: 0.353
[epoch  3, batch   200] loss: 0.359
[epoch  3, batch   250] loss: 0.345
[epoch  3, batch   300] loss: 0.336
[epoch  3, batch   350] loss: 0.310
[epoch  3, batch   400] loss: 0.326
[epoch  3, batch   450] loss: 0.293
[epoch  3, batch   500] loss: 0.283
[epoch  3, batch   550] loss: 0.330
[epoch  3, batch   600] loss: 0.299
[epoch  3, batch   650] loss: 0.302
[epoch  3, batch   700] loss: 0.275
[epoch  3, batch   750] loss: 0.302
[epoch  3, batch   800] loss: 0.269
[epoch  3, batch   850] loss: 0.305
[epoch  3, batch   900] loss: 0.258
[epoch  4, batch    50] loss: 0.274
[epoch  4, batch   100] loss: 0.265
[epoch  4, batch   150] loss: 0.265
[epoch  4, batch   200] loss: 0.245
[epoch  4, batch   250] loss: 0.243
[epoch  4, batch   300] loss: 0.251
[epoch  4, batch   350] loss: 0.230
[epoch  4, batch   400] loss: 0.250
[epoch  4, batch   450] loss: 0.260
[epoch  4, batch   500] loss: 0.225
[epoch  4, batch   550] loss: 0.235
[epoch  4, batch   600] loss: 0.265
[epoch  4, batch   650] loss: 0.259
[epoch  4, batch   700] loss: 0.261
[epoch  4, batch   750] loss: 0.226
[epoch  4, batch   800] loss: 0.249
[epoch  4, batch   850] loss: 0.236
[epoch  4, batch   900] loss: 0.249
[epoch  5, batch    50] loss: 0.213
[epoch  5, batch   100] loss: 0.218
[epoch  5, batch   150] loss: 0.202
[epoch  5, batch   200] loss: 0.217
[epoch  5, batch   250] loss: 0.227
[epoch  5, batch   300] loss: 0.234
[epoch  5, batch   350] loss: 0.198
[epoch  5, batch   400] loss: 0.232
[epoch  5, batch   450] loss: 0.212
[epoch  5, batch   500] loss: 0.203
[epoch  5, batch   550] loss: 0.194
[epoch  5, batch   600] loss: 0.208
[epoch  5, batch   650] loss: 0.193
[epoch  5, batch   700] loss: 0.195
[epoch  5, batch   750] loss: 0.209
[epoch  5, batch   800] loss: 0.202
[epoch  5, batch   850] loss: 0.223
[epoch  5, batch   900] loss: 0.198
[epoch  6, batch    50] loss: 0.189
[epoch  6, batch   100] loss: 0.213
[epoch  6, batch   150] loss: 0.173
[epoch  6, batch   200] loss: 0.153
[epoch  6, batch   250] loss: 0.197
[epoch  6, batch   300] loss: 0.195
[epoch  6, batch   350] loss: 0.171
[epoch  6, batch   400] loss: 0.188
[epoch  6, batch   450] loss: 0.194
[epoch  6, batch   500] loss: 0.183
[epoch  6, batch   550] loss: 0.173
[epoch  6, batch   600] loss: 0.181
[epoch  6, batch   650] loss: 0.184
[epoch  6, batch   700] loss: 0.203
[epoch  6, batch   750] loss: 0.209
[epoch  6, batch   800] loss: 0.166
[epoch  6, batch   850] loss: 0.194
[epoch  6, batch   900] loss: 0.158
[epoch  7, batch    50] loss: 0.164
[epoch  7, batch   100] loss: 0.155
[epoch  7, batch   150] loss: 0.154
[epoch  7, batch   200] loss: 0.184
[epoch  7, batch   250] loss: 0.167
[epoch  7, batch   300] loss: 0.158
[epoch  7, batch   350] loss: 0.155
[epoch  7, batch   400] loss: 0.150
[epoch  7, batch   450] loss: 0.147
[epoch  7, batch   500] loss: 0.162
[epoch  7, batch   550] loss: 0.166
[epoch  7, batch   600] loss: 0.158
[epoch  7, batch   650] loss: 0.164
[epoch  7, batch   700] loss: 0.163
[epoch  7, batch   750] loss: 0.157
[epoch  7, batch   800] loss: 0.156
[epoch  7, batch   850] loss: 0.150
[epoch  7, batch   900] loss: 0.140
[epoch  8, batch    50] loss: 0.139
[epoch  8, batch   100] loss: 0.153
[epoch  8, batch   150] loss: 0.141
[epoch  8, batch   200] loss: 0.133
[epoch  8, batch   250] loss: 0.134
[epoch  8, batch   300] loss: 0.133
[epoch  8, batch   350] loss: 0.131
[epoch  8, batch   400] loss: 0.136
[epoch  8, batch   450] loss: 0.137
[epoch  8, batch   500] loss: 0.130
[epoch  8, batch   550] loss: 0.131
[epoch  8, batch   600] loss: 0.137
[epoch  8, batch   650] loss: 0.135
[epoch  8, batch   700] loss: 0.125
[epoch  8, batch   750] loss: 0.133
[epoch  8, batch   800] loss: 0.146
[epoch  8, batch   850] loss: 0.122
[epoch  8, batch   900] loss: 0.131
[epoch  9, batch    50] loss: 0.133
[epoch  9, batch   100] loss: 0.122
[epoch  9, batch   150] loss: 0.130
[epoch  9, batch   200] loss: 0.117
[epoch  9, batch   250] loss: 0.104
[epoch  9, batch   300] loss: 0.131
[epoch  9, batch   350] loss: 0.120
[epoch  9, batch   400] loss: 0.108
[epoch  9, batch   450] loss: 0.127
[epoch  9, batch   500] loss: 0.131
[epoch  9, batch   550] loss: 0.119
[epoch  9, batch   600] loss: 0.120
[epoch  9, batch   650] loss: 0.126
[epoch  9, batch   700] loss: 0.101
[epoch  9, batch   750] loss: 0.114
[epoch  9, batch   800] loss: 0.109
[epoch  9, batch   850] loss: 0.122
[epoch  9, batch   900] loss: 0.104
[epoch 10, batch    50] loss: 0.101
[epoch 10, batch   100] loss: 0.112
[epoch 10, batch   150] loss: 0.114
[epoch 10, batch   200] loss: 0.102
[epoch 10, batch   250] loss: 0.113
[epoch 10, batch   300] loss: 0.098
[epoch 10, batch   350] loss: 0.105
[epoch 10, batch   400] loss: 0.103
[epoch 10, batch   450] loss: 0.112
[epoch 10, batch   500] loss: 0.101
[epoch 10, batch   550] loss: 0.101
[epoch 10, batch   600] loss: 0.130
[epoch 10, batch   650] loss: 0.111
[epoch 10, batch   700] loss: 0.109
[epoch 10, batch   750] loss: 0.088
[epoch 10, batch   800] loss: 0.116
[epoch 10, batch   850] loss: 0.105
[epoch 10, batch   900] loss: 0.105
Finished Training
GroundTruth:  7 - seven 2 - two 1 - one 0 - zero 4 - four 1 - one 4 - four 9 - nine 5 - five 9 - nine 0 - zero 6 - six 9 - nine 0 - zero 1 - one 5 - five 9 - nine 7 - seven 3 - three 4 - four 9 - nine 6 - six 6 - six 5 - five 4 - four 0 - zero 7 - seven 4 - four 0 - zero 1 - one 3 - three 1 - one 3 - three 4 - four 7 - seven 2 - two 7 - seven 1 - one 2 - two 1 - one 1 - one 7 - seven 4 - four 2 - two 3 - three 5 - five 1 - one 2 - two 4 - four 4 - four 6 - six 3 - three 5 - five 5 - five 6 - six 0 - zero 4 - four 1 - one 9 - nine 5 - five 7 - seven 8 - eight 9 - nine 3 - three
Prediction:  7 - seven 2 - two 1 - one 0 - zero 4 - four 1 - one 4 - four 9 - nine 5 - five 9 - nine 0 - zero 6 - six 9 - nine 0 - zero 1 - one 5 - five 9 - nine 7 - seven 3 - three 4 - four 9 - nine 6 - six 6 - six 5 - five 4 - four 0 - zero 7 - seven 4 - four 0 - zero 1 - one 3 - three 1 - one 3 - three 4 - four 7 - seven 2 - two 7 - seven 1 - one 2 - two 1 - one 1 - one 7 - seven 4 - four 2 - two 3 - three 5 - five 1 - one 2 - two 4 - four 4 - four 6 - six 3 - three 5 - five 5 - five 6 - six 0 - zero 4 - four 1 - one 9 - nine 5 - five 7 - seven 8 - eight 9 - nine 3 - three
Model saved to model.pth.
Accuracy on the train set: 96.741667 %
Accuracy on the test set: 96.770000 %
Student ID: 21009536G