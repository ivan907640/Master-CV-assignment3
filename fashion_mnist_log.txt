Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0
[epoch  1, batch    50] loss: 1.370
[epoch  1, batch   100] loss: 0.628
[epoch  1, batch   150] loss: 0.498
[epoch  1, batch   200] loss: 0.430
[epoch  1, batch   250] loss: 0.415
[epoch  1, batch   300] loss: 0.362
[epoch  1, batch   350] loss: 0.342
[epoch  1, batch   400] loss: 0.329
[epoch  1, batch   450] loss: 0.336
[epoch  1, batch   500] loss: 0.314
[epoch  1, batch   550] loss: 0.289
[epoch  1, batch   600] loss: 0.282
[epoch  1, batch   650] loss: 0.287
[epoch  1, batch   700] loss: 0.268
[epoch  1, batch   750] loss: 0.276
[epoch  1, batch   800] loss: 0.264
[epoch  1, batch   850] loss: 0.262
[epoch  1, batch   900] loss: 0.254
[epoch  2, batch    50] loss: 0.222
[epoch  2, batch   100] loss: 0.231
[epoch  2, batch   150] loss: 0.241
[epoch  2, batch   200] loss: 0.213
[epoch  2, batch   250] loss: 0.230
[epoch  2, batch   300] loss: 0.236
[epoch  2, batch   350] loss: 0.227
[epoch  2, batch   400] loss: 0.214
[epoch  2, batch   450] loss: 0.205
[epoch  2, batch   500] loss: 0.211
[epoch  2, batch   550] loss: 0.207
[epoch  2, batch   600] loss: 0.210
[epoch  2, batch   650] loss: 0.180
[epoch  2, batch   700] loss: 0.207
[epoch  2, batch   750] loss: 0.207
[epoch  2, batch   800] loss: 0.234
[epoch  2, batch   850] loss: 0.214
[epoch  2, batch   900] loss: 0.222
[epoch  3, batch    50] loss: 0.178
[epoch  3, batch   100] loss: 0.178
[epoch  3, batch   150] loss: 0.181
[epoch  3, batch   200] loss: 0.170
[epoch  3, batch   250] loss: 0.161
[epoch  3, batch   300] loss: 0.171
[epoch  3, batch   350] loss: 0.183
[epoch  3, batch   400] loss: 0.167
[epoch  3, batch   450] loss: 0.163
[epoch  3, batch   500] loss: 0.178
[epoch  3, batch   550] loss: 0.193
[epoch  3, batch   600] loss: 0.169
[epoch  3, batch   650] loss: 0.175
[epoch  3, batch   700] loss: 0.170
[epoch  3, batch   750] loss: 0.158
[epoch  3, batch   800] loss: 0.184
[epoch  3, batch   850] loss: 0.169
[epoch  3, batch   900] loss: 0.185
[epoch  4, batch    50] loss: 0.145
[epoch  4, batch   100] loss: 0.135
[epoch  4, batch   150] loss: 0.136
[epoch  4, batch   200] loss: 0.135
[epoch  4, batch   250] loss: 0.145
[epoch  4, batch   300] loss: 0.148
[epoch  4, batch   350] loss: 0.141
[epoch  4, batch   400] loss: 0.142
[epoch  4, batch   450] loss: 0.133
[epoch  4, batch   500] loss: 0.154
[epoch  4, batch   550] loss: 0.130
[epoch  4, batch   600] loss: 0.151
[epoch  4, batch   650] loss: 0.125
[epoch  4, batch   700] loss: 0.144
[epoch  4, batch   750] loss: 0.137
[epoch  4, batch   800] loss: 0.136
[epoch  4, batch   850] loss: 0.149
[epoch  4, batch   900] loss: 0.147
[epoch  5, batch    50] loss: 0.110
[epoch  5, batch   100] loss: 0.120
[epoch  5, batch   150] loss: 0.115
[epoch  5, batch   200] loss: 0.119
[epoch  5, batch   250] loss: 0.123
[epoch  5, batch   300] loss: 0.110
[epoch  5, batch   350] loss: 0.109
[epoch  5, batch   400] loss: 0.118
[epoch  5, batch   450] loss: 0.127
[epoch  5, batch   500] loss: 0.107
[epoch  5, batch   550] loss: 0.109
[epoch  5, batch   600] loss: 0.123
[epoch  5, batch   650] loss: 0.122
[epoch  5, batch   700] loss: 0.128
[epoch  5, batch   750] loss: 0.136
[epoch  5, batch   800] loss: 0.105
[epoch  5, batch   850] loss: 0.113
[epoch  5, batch   900] loss: 0.116
[epoch  6, batch    50] loss: 0.083
[epoch  6, batch   100] loss: 0.100
[epoch  6, batch   150] loss: 0.086
[epoch  6, batch   200] loss: 0.081
[epoch  6, batch   250] loss: 0.089
[epoch  6, batch   300] loss: 0.087
[epoch  6, batch   350] loss: 0.088
[epoch  6, batch   400] loss: 0.092
[epoch  6, batch   450] loss: 0.096
[epoch  6, batch   500] loss: 0.098
[epoch  6, batch   550] loss: 0.099
[epoch  6, batch   600] loss: 0.102
[epoch  6, batch   650] loss: 0.099
[epoch  6, batch   700] loss: 0.085
[epoch  6, batch   750] loss: 0.100
[epoch  6, batch   800] loss: 0.098
[epoch  6, batch   850] loss: 0.100
[epoch  6, batch   900] loss: 0.102
[epoch  7, batch    50] loss: 0.080
[epoch  7, batch   100] loss: 0.075
[epoch  7, batch   150] loss: 0.071
[epoch  7, batch   200] loss: 0.078
[epoch  7, batch   250] loss: 0.077
[epoch  7, batch   300] loss: 0.067
[epoch  7, batch   350] loss: 0.070
[epoch  7, batch   400] loss: 0.057
[epoch  7, batch   450] loss: 0.063
[epoch  7, batch   500] loss: 0.077
[epoch  7, batch   550] loss: 0.084
[epoch  7, batch   600] loss: 0.070
[epoch  7, batch   650] loss: 0.069
[epoch  7, batch   700] loss: 0.074
[epoch  7, batch   750] loss: 0.086
[epoch  7, batch   800] loss: 0.064
[epoch  7, batch   850] loss: 0.071
[epoch  7, batch   900] loss: 0.086
[epoch  8, batch    50] loss: 0.052
[epoch  8, batch   100] loss: 0.048
[epoch  8, batch   150] loss: 0.063
[epoch  8, batch   200] loss: 0.043
[epoch  8, batch   250] loss: 0.051
[epoch  8, batch   300] loss: 0.050
[epoch  8, batch   350] loss: 0.054
[epoch  8, batch   400] loss: 0.056
[epoch  8, batch   450] loss: 0.067
[epoch  8, batch   500] loss: 0.053
[epoch  8, batch   550] loss: 0.066
[epoch  8, batch   600] loss: 0.058
[epoch  8, batch   650] loss: 0.050
[epoch  8, batch   700] loss: 0.066
[epoch  8, batch   750] loss: 0.056
[epoch  8, batch   800] loss: 0.058
[epoch  8, batch   850] loss: 0.059
[epoch  8, batch   900] loss: 0.059
[epoch  9, batch    50] loss: 0.040
[epoch  9, batch   100] loss: 0.036
[epoch  9, batch   150] loss: 0.038
[epoch  9, batch   200] loss: 0.037
[epoch  9, batch   250] loss: 0.031
[epoch  9, batch   300] loss: 0.033
[epoch  9, batch   350] loss: 0.033
[epoch  9, batch   400] loss: 0.039
[epoch  9, batch   450] loss: 0.045
[epoch  9, batch   500] loss: 0.045
[epoch  9, batch   550] loss: 0.044
[epoch  9, batch   600] loss: 0.042
[epoch  9, batch   650] loss: 0.044
[epoch  9, batch   700] loss: 0.040
[epoch  9, batch   750] loss: 0.044
[epoch  9, batch   800] loss: 0.038
[epoch  9, batch   850] loss: 0.048
[epoch  9, batch   900] loss: 0.051
[epoch 10, batch    50] loss: 0.036
[epoch 10, batch   100] loss: 0.029
[epoch 10, batch   150] loss: 0.026
[epoch 10, batch   200] loss: 0.023
[epoch 10, batch   250] loss: 0.028
[epoch 10, batch   300] loss: 0.030
[epoch 10, batch   350] loss: 0.033
[epoch 10, batch   400] loss: 0.033
[epoch 10, batch   450] loss: 0.030
[epoch 10, batch   500] loss: 0.027
[epoch 10, batch   550] loss: 0.028
[epoch 10, batch   600] loss: 0.026
[epoch 10, batch   650] loss: 0.029
[epoch 10, batch   700] loss: 0.027
[epoch 10, batch   750] loss: 0.035
[epoch 10, batch   800] loss: 0.038
[epoch 10, batch   850] loss: 0.032
[epoch 10, batch   900] loss: 0.034
Finished Training
GroundTruth:  Ankle boot Pullover Trouser Trouser Shirt Trouser  Coat Shirt Sandal Sneaker  Coat Sandal Sneaker Dress  Coat Trouser Pullover  Coat   Bag T-shirt/top Pullover Sandal Sneaker Ankle boot Trouser  Coat Shirt T-shirt/top Ankle boot Dress   Bag   Bag Dress Dress   Bag T-shirt/top Sneaker Sandal Sneaker Ankle boot Shirt Trouser Dress Sneaker Shirt Sneaker Pullover Trouser Pullover Pullover  Coat  Coat Sandal   Bag Pullover Pullover   Bag  Coat   Bag T-shirt/top Sneaker Sneaker   Bag Sandal
Prediction:  Sneaker Shirt Dress Trouser Ankle boot Shirt Shirt Pullover Pullover Ankle boot Shirt Ankle boot Pullover Pullover T-shirt/top Trouser Pullover Shirt Shirt Pullover Trouser Ankle boot   Bag T-shirt/top T-shirt/top Shirt Shirt Pullover T-shirt/top Pullover  Coat Shirt T-shirt/top T-shirt/top Dress  Coat Ankle boot Pullover Shirt Shirt Sneaker Dress Pullover Shirt Pullover Shirt Pullover Dress Shirt  Coat T-shirt/top Shirt  Coat Trouser Pullover Dress Pullover Pullover Ankle boot Trouser Ankle boot   Bag Shirt T-shirt/top
Model saved to fashion_mnist.pth.
Accuracy on the train set: 98.255000 %
Accuracy on the test set: 92.890000 %